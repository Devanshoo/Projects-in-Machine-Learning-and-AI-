{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3201a1bd",
   "metadata": {},
   "source": [
    "# Homework 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fbb87",
   "metadata": {},
   "source": [
    "### Task 3 (100 points)-Only for Graduate Students:\n",
    "\n",
    "This part of homework 5 is focused on reinforcement learning (RL) and is due on November 12th (11:59 pm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fced60",
   "metadata": {},
   "source": [
    "#### Task 3.1(20 points):  We discussed how we can formulate RL problems as an MDP. Describe any real-world application that can be formulated as an MDP. Describe what the state space, action space, transition model, and reward are. You do not ne ed to be precise in the description of the transition model and reward (no formula is needed). Qualitative description is enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711814cc",
   "metadata": {},
   "source": [
    "In a Reinforcement Learning problem, we usually have a learner and a decision maker which are called agent and the surrounding with which it interacts is called the environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc9263",
   "metadata": {},
   "source": [
    "So, in an MDP, an agent interacts with an environment by taking actions and seek to maximize the rewards the agent gets from the environment. At any given time stamp t, the process is as follows:\n",
    "\n",
    "1. The environment is in state St\n",
    "2. The agent takes an action At\n",
    "3. The environment generates a reward Rt based on St and At\n",
    "4. The environment moves to the next state St+1\n",
    "\n",
    "The goal of this agent is to increase and maximize the total rewards (ΣRt) gathered over a period of time. The agent needs to find best possible action on a given state that will maximize the total reward. The probability distribution of taking actions At from a state St is called policy π(At | St). The goal of solving an MDP is to find the optimal policy.\n",
    "\n",
    "To express a problem using MDP, one needs to define the followings:\n",
    "\n",
    "1. states of the environment\n",
    "2. actions the agent can take on each state\n",
    "3. rewards obtained after taking an action on a given state\n",
    "4. state transition probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb8154",
   "metadata": {},
   "source": [
    "I studied multiple examples but found this medium article with an example to fish salmons which I found really interesting and easy to explain:\n",
    "\n",
    "#### Real World Example:  Whether to fish salmons this year\n",
    "\n",
    "In this example the proportion of salmons are decided which can be catched in a year provided it maximizes the reward. Each salmon is considered to generate a fixed amount of dollar. But if a large proportion of salmons are caught then the yield of the next year will reduce which is the penalty. \n",
    "\n",
    "We need to find the optimum portion of salmons to catch to maximize the return over a long time period.\n",
    "\n",
    "The problem is then simplified to whether to fish a certain portion of salmon or not. This problem can be expressed as an MDP as follows\n",
    "\n",
    "States: The number of salmons available in that area in that year. \n",
    "\n",
    "For simplicity it is assumed that there are only 4 states; empty, low, medium, high. The four states are defined as follows\n",
    "\n",
    "Empty -> no salmons are available; \n",
    "low -> available number of salmons are below a certain threshold t1; \n",
    "medium -> available number of salmons are between t1 and t2; \n",
    "high -> available number of salmons are more than t2\n",
    "\n",
    "Actions: 2 actions are assumed; fish and not_to_fish.\n",
    "\n",
    "Rewards: Fishing at certain state generates following rewards: \n",
    "Low, medium and high are $5K, $50K and $100k respectively. \n",
    "\n",
    "State Transitions: Fishing in a state has higher a probability to move to a state with lower number of salmons. Not_to_fish action has higher probability to move to a state with higher number of salmons \n",
    "\n",
    "Now, there are two kinds of nodes considered:\n",
    "Large circles are state nodes, small solid black circles are action nodes. \n",
    "Once an action is taken the environment responds with a reward and transitions to the next state. \n",
    "\n",
    "Each arrow shows the <transition probability, reward>. \n",
    "\n",
    "For example, a Medium action node Fish has 2 arrows transitioning to 2 different states; i) Low with (probability=0.75, reward=$10K) or ii) back to Medium with (probability=0.25, reward=$10K). \n",
    "In the state Empty, the only action is Re-breed which transitions to the state Low with (probability=1, reward=-$200K).\n",
    "\n",
    "https://miro.medium.com/max/1400/1*6q1Lyrm7VRkBJitaUIc0DA.png![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfec36",
   "metadata": {},
   "source": [
    "#### Task 3.2 (30 points): RL is used in various sectors - Healthcare, recommender systems and trading are a few of those. Pick one of the three areas. Explain one of the problems in any of these domains that can be more effectively solved by reinforcement learning. Find an open-source project (if any) that has addressed this problem. Explain this project in detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836aca3",
   "metadata": {},
   "source": [
    " USING REINFORCEMENT LEARNING FOR MEDICAL IMAGE THRESHOLDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2efbf",
   "metadata": {},
   "source": [
    "I chose the example of image Thresholding which talks image processing techniques.\n",
    "\n",
    "In several applications, due to non-optimal circumstances like noise, non-uniform illumination etc., optimal results cannot be achieved. This particular paper in its solution proposes to solve the problem by designing and implementing a reinforcement learning agent. \n",
    "\n",
    "Here, the agent will learn the optimal threshold for an image through interactions with an experienced user,or integrating objective domain knowledge. The agent will discover which actions give the maximum reward based on a predefmed reward and punishment function. This reward/punishment can be objective or subjective. This method is a general approach to image thresholding.\n",
    "\n",
    "To separate the object gray levels go from the background gray levels, we need to determine a proper threshold T. Optimal thresholds can usually he extracted 60m bimodal histograms.\n",
    "\n",
    "The reinforcement learning agent is designed based on characteristics of the problem. The most important concepts of reinforcement learning agent are state ob- servation, taking appropriate actions, and receiving re- wards.\n",
    "\n",
    "The interaction of user with the agent in this method is based on using a GUI. By using this GUI the agent can talk with an experienced operator. The agent takes samples 60M as a set of test images and thresholds them. The proposed agent uses 50 different states defined based on following parameter.\n",
    "\n",
    "Further, Q-learning algoriyhm is used. The Q-matrix, the matrix of accumulated reward, is implemented based on the actions and the states to find out maximum probability of taking each action according to Boltanan policy.\n",
    "\n",
    "The agent modifies the initial threshold by taking an action. A total number of 20 actions have been defined for this purpose.\n",
    "\n",
    "Reward and Punishment:\n",
    "\n",
    "The reinforcement agent receives two types of reward and punishment, objective and subjective. In subjective case they will receive the reward and punishment directly from the user. This task is done by interaction with agent through the GUI. The user will be asked whether the result of the thresholding is \"bad, \"fair\" or \"good\". The user will choose one of these options to give reward and punishment based on hisher experience.\n",
    "\n",
    "The algorithm was implemented in MatLab where in objective case the user determines the number of objects, the numher of holes, an estimate of object area and tolerance for area deviation. The user also defines the number of iteration, and the values. A typical initial value for temperature was 100.\n",
    "\n",
    "Citation: M. Shokri and H. R. Tizhoosh, \"Using reinforcement learning for image thresholding,\" CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436), 2003, pp. 1231-1234 vol.2, doi: 10.1109/CCECE.2003.1226121."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb0a2f",
   "metadata": {},
   "source": [
    "### Task 3.3 (50 points): Implement the game of tic-tac-toe (write a class that implements an agent playing Tic Tac Toe and learning its Q function) using the Q-learning technique (see the resource provided in class for more details). Clearly describe your evaluation metric and demonstrate a few runs. You might need to use some online resources to proceed on this. Do not forget to cite those.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0831cc",
   "metadata": {},
   "source": [
    "I used the following website to thoroughly understand the implementation of the game TIC TAC TOE and re-implemented using similar process: https://techvidvan.com/tutorials/python-game-project-tic-tac-toe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060775f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.8.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame as pg,sys\n",
    "from pygame.locals import *\n",
    "import time\n",
    "\n",
    "\n",
    "#initialize global variables\n",
    "XO = 'x'\n",
    "winner = None\n",
    "draw = False\n",
    "width = 400\n",
    "height = 400\n",
    "white = (255, 255, 255)\n",
    "line_color = (10,10,10)\n",
    "\n",
    "#TicTacToe 3x3 board\n",
    "board = [[None]*3,[None]*3,[None]*3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8fc336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing pygame window\n",
    "pg.init()\n",
    "fps = 60\n",
    "CLOCK = pg.time.Clock()\n",
    "screen = pg.display.set_mode((width, height+100),0,32)\n",
    "pg.display.set_caption(\"Tic Tac Toe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdf0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the images\n",
    "opening = pg.image.load('/Users/devanshoojain/RPI_Sem_2/Projects_in_AI_ML/Homeworks/Homework_5/tic-tac-toe-python-project/tic tac opening.png')\n",
    "x_img = pg.image.load('/Users/devanshoojain/RPI_Sem_2/Projects_in_AI_ML/Homeworks/Homework_5/tic-tac-toe-python-project/x.png')\n",
    "o_img = pg.image.load('/Users/devanshoojain/RPI_Sem_2/Projects_in_AI_ML/Homeworks/Homework_5/tic-tac-toe-python-project/o.png')\n",
    "\n",
    "#resizing images\n",
    "x_img = pg.transform.scale(x_img, (80,80))\n",
    "o_img = pg.transform.scale(o_img, (80,80))\n",
    "opening = pg.transform.scale(opening, (width, height+100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42adb93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_opening():\n",
    "    screen.blit(opening,(0,0))\n",
    "    pg.display.update()\n",
    "    time.sleep(1)\n",
    "    screen.fill(white)\n",
    "\n",
    "    # Drawing vertical lines\n",
    "    pg.draw.line(screen,line_color,(width/3,0),(width/3, height),7)\n",
    "    pg.draw.line(screen,line_color,(width/3*2,0),(width/3*2, height),7)\n",
    "    # Drawing horizontal lines\n",
    "    pg.draw.line(screen,line_color,(0,height/3),(width, height/3),7)\n",
    "    pg.draw.line(screen,line_color,(0,height/3*2),(width, height/3*2),7)\n",
    "    draw_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185a9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_status():\n",
    "    global draw\n",
    "\n",
    "    if winner is None:\n",
    "        message = XO.upper() + \"'s Turn\"\n",
    "    else:\n",
    "        message = winner.upper() + \" won!\"\n",
    "    if draw:\n",
    "        message = 'Game Draw!'\n",
    "\n",
    "    font = pg.font.Font(None, 30)\n",
    "    text = font.render(message, 1, (255, 255, 255))\n",
    "\n",
    "    # copy the rendered message onto the board\n",
    "    screen.fill ((0, 0, 0), (0, 400, 500, 100))\n",
    "    text_rect = text.get_rect(center=(width/2, 500-50))\n",
    "    screen.blit(text, text_rect)\n",
    "    pg.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8018430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_win():\n",
    "    global board, winner, draw\n",
    "\n",
    "    # check for winning rows\n",
    "    for row in range (0,3):\n",
    "        if ((board [row][0] == board[row][1] == board[row][2]) and(board [row][0] is not None)):\n",
    "            # this row won\n",
    "            winner = board[row][0]\n",
    "            pg.draw.line(screen, (250,0,0), (0, (row + 1)*height/3 -height/6),\\\n",
    "                              (width, (row + 1)*height/3 - height/6 ), 4)\n",
    "            break\n",
    "\n",
    "    # check for winning columns\n",
    "    for col in range (0, 3):\n",
    "        if (board[0][col] == board[1][col] == board[2][col]) and (board[0][col] is not None):\n",
    "            # this column won\n",
    "            winner = board[0][col]\n",
    "            #draw winning line\n",
    "            pg.draw.line (screen, (250,0,0),((col + 1)* width/3 - width/6, 0),\\\n",
    "                          ((col + 1)* width/3 - width/6, height), 4)\n",
    "            break\n",
    "\n",
    "    # check for diagonal winners\n",
    "    if (board[0][0] == board[1][1] == board[2][2]) and (board[0][0] is not None):\n",
    "        # game won diagonally left to right\n",
    "        winner = TTT[0][0]\n",
    "        pg.draw.line (screen, (250,70,70), (50, 50), (350, 350), 4)\n",
    "\n",
    "    if (board[0][2] == board[1][1] == board[2][0]) and (board[0][2] is not None):\n",
    "        # game won diagonally right to left\n",
    "        winner = board[0][2]\n",
    "        pg.draw.line (screen, (250,70,70), (350, 50), (50, 350), 4)\n",
    "\n",
    "    if(all([all(row) for row in board]) and winner is None ):\n",
    "        draw = True\n",
    "    draw_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a86849a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawXO(row,col):\n",
    "    global board, XO\n",
    "    if row==1:\n",
    "        posx = 30\n",
    "    if row==2:\n",
    "        posx = width/3 + 30\n",
    "    if row==3:\n",
    "        posx = width/3*2 + 30\n",
    "\n",
    "    if col==1:\n",
    "        posy = 30\n",
    "    if col==2:\n",
    "        posy = height/3 + 30\n",
    "    if col==3:\n",
    "        posy = height/3*2 + 30\n",
    "    board[row-1][col-1] = XO\n",
    "    if(XO == 'x'):\n",
    "        screen.blit(x_img,(posy,posx))\n",
    "        XO= 'o'\n",
    "    else:\n",
    "        screen.blit(o_img,(posy,posx))\n",
    "        XO= 'x'\n",
    "    pg.display.update()\n",
    "    #print(posx,posy)\n",
    "    #print(TTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d595c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def userClick():\n",
    "    #get coordinates of mouse click\n",
    "    x, y = pg.mouse.get_pos()\n",
    "\n",
    "    #get column of mouse click (1-3)\n",
    "    if(x<width/3):\n",
    "        col = 1\n",
    "    elif (x<width/3*2):\n",
    "        col = 2\n",
    "    elif(x<width):\n",
    "        col = 3\n",
    "    else:\n",
    "        col = None\n",
    "\n",
    "    #get row of mouse click (1-3)\n",
    "    if(y<height/3):\n",
    "        row = 1\n",
    "    elif (y<height/3*2):\n",
    "        row = 2\n",
    "    elif(y<height):\n",
    "        row = 3\n",
    "    else:\n",
    "        row = None\n",
    "    #print(row,col)\n",
    "\n",
    "    if(row and col and board[row-1][col-1] is None):\n",
    "        global XO\n",
    "\n",
    "        #draw the x or o on screen\n",
    "        drawXO(row,col)\n",
    "        check_win()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8080463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_game():\n",
    "    global board, winner,XO, draw\n",
    "    time.sleep(3)\n",
    "    XO = 'x'\n",
    "    draw = False\n",
    "    game_opening()\n",
    "    winner=None\n",
    "    board = [[None]*3,[None]*3,[None]*3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa460f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "game_opening()\n",
    "\n",
    "# run the game loop forever\n",
    "while(True):\n",
    "    for event in pg.event.get():\n",
    "        if event.type == QUIT:\n",
    "            pg.quit()\n",
    "            sys.exit()\n",
    "        elif event.type == MOUSEBUTTONDOWN:\n",
    "            # the user clicked; place an X or O\n",
    "            userClick()\n",
    "            if(winner or draw):\n",
    "                reset_game()\n",
    "\n",
    "    pg.display.update()\n",
    "    CLOCK.tick(fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9010b6c",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "    1. https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\n",
    "    2. https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "    3. https://techvidvan.com/tutorials/python-game-project-tic-tac-toe/\n",
    "    4. https://www.tensorflow.org/tutorials/generative/cvae\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
